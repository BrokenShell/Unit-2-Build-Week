{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package and Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'category_encoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3462d29e395b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from category_encoders import OneHotEncoder\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "df = pd.read_csv('../assets/churn_ds.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-65485bbe80eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Graphing features for exploratory analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Contract'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tenure'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Churn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "# Graphing features for exploratory analysis\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "px.bar(df, x='Contract', y='tenure', color='Churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Train/Validate/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_float(cell):\n",
    "  try: \n",
    "    return float(cell)\n",
    "  except: \n",
    "    print(cell)\n",
    "    return np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature / Target Split \n",
    "- By using this set of data our goal is to predict if a customer is likely to continue service with the provider. The **Target** will be if the customer has Churned - that means they have discontinued service. Churn is a term used in marketing to describe when a customer comes into the system and leaves rather quickly, yeild a low CLV. The Feature we will be using to predict this will be a combination of person charateristics: Age, Gender, Family. Service offers they currently have with the provider: Phone Service, Internet Service, Online Security, Streaming TV, etc. Also Subscription details: Contract, Billing, Payment Method, Monthly and Total Charges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Wrangle function \n",
    "\n",
    "def wrangle(X):\n",
    "    \n",
    "    X = X.copy()\n",
    "    \n",
    "    # fixing column to change to float\n",
    "    X['TotalCharges'] = X['TotalCharges'].apply(fix_float)\n",
    "                                                  \n",
    "    # replacing Yes/No with True/False\n",
    "    columns = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']                                              \n",
    "    for col in columns:\n",
    "        X[col] = X[col].apply(lambda cell: cell.lower() == 'yes')\n",
    "\n",
    "    y = X['Churn']    \n",
    "        \n",
    "    X.drop(['customerID', 'Churn'], axis=1, inplace=True)\n",
    "                                                  \n",
    "    return X, y\n",
    "\n",
    "X, y = wrangle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sort_values(by='Importances', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Val split \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.30, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val test split \n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=.50, stratify=y_val, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for CV sizes\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline for Churn\n",
    "- From the above baseline we can see our mean values for predicting the positive charatertic of Churn is 73.5%. Our Goal is to create a model in which we can predict to a higher level of certaintity if a customer will decide to terminate service. As well as what features are indicative of this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Different Model Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Simple Model: Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = make_pipeline(\n",
    "    ce.OneHotEncoder(), \n",
    "    SimpleImputer(strategy='median'), \n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "lin_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy: ', lin_model.score(X_train, y_train))\n",
    "print('Validation Accuracy:', lin_model.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter Tuning for Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pipeline for model\n",
    "lin_model_tuned = make_pipeline(\n",
    "    ce.OneHotEncoder(), \n",
    "    SimpleImputer(), \n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "# Params for grid search\n",
    "params = {\n",
    "    'logisticregression__penalty' : ['l1', 'l2'],\n",
    "    'logisticregression__C' : np.logspace(-4, 4, 20), \n",
    "    'logisticregression__solver': ['lbfgs', 'liblinear'], \n",
    "    'logisticregression__max_iter': range(50,150, 25), \n",
    "}\n",
    "\n",
    "# Grid Search object\n",
    "gridcv = GridSearchCV(\n",
    "    lin_model_tuned,\n",
    "    param_grid = params, \n",
    "    n_jobs=-1, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    verbose= True, \n",
    "    return_train_score= True,\n",
    "\n",
    ")\n",
    "\n",
    "# fit on data\n",
    "best_gridcv = gridcv.fit(X_train, y_train)\n",
    "\n",
    "# Print Best accuracy score\n",
    "print(best_gridcv.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After Tuning the model, it looks like the accuracy has improved just slightly. This leads us "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic pipeline model\n",
    "RF_model = make_pipeline(\n",
    "    ce.OneHotEncoder(), \n",
    "    SimpleImputer(), \n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "RF_model.fit(X_train, y_train)\n",
    "RF_model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "y_pred_prob_rf = RF_model.predict_proba(X_val)[:, -1]\n",
    "\n",
    "roc_auc_score(y_val, y_pred_prob_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Due to initual Validation score being very low compared to other models, Not going to further tune this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Based Model + Hyper Param tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = make_pipeline(\n",
    "    ce.OneHotEncoder(),\n",
    "    XGBClassifier(n_estimators=100, random_state=42, n_jobs=6)\n",
    ")\n",
    "\n",
    "param_distributions= {\n",
    "    'xgbclassifier__max_depth': range(4,6,1), \n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    tree_model,\n",
    "    param_distributions=param_distributions, \n",
    "    n_iter=10, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    verbose= 5, \n",
    "    return_train_score= True,\n",
    "\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "tree_model_best = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model_best.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sticking with Logistic Regression seems like the way to go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_lin = best_gridcv.predict(X_val)\n",
    "\n",
    "report1 = classification_report(y_val, y_pred_lin)\n",
    "print(report1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_XG = tree_model_best.predict(X_val)\n",
    "\n",
    "report2 = classification_report(y_val, y_pred_XG)\n",
    "print(report2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_RF = RF_model.predict(X_val)\n",
    "\n",
    "report3 = classification_report(y_val, y_pred_RF)\n",
    "print(report3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC - AUC Curves + Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking into ROC-AUC score for Linear Regression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred_prob = best_gridcv.predict_proba(X_val)[:, -1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred_prob)\n",
    "\n",
    "dfroc = pd.DataFrame({'False Positive Rate': fpr, \n",
    "                     'True Positive Rate': tpr,\n",
    "                    'Threshold': np.round(thresholds, 2)})\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "print('LR Model ROC-AUC Score:', roc_auc_score(y_val, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining ROC + COmparing it to LR model\n",
    "y_pred_prob_t = tree_model_best.predict_proba(X_val)[:, -1]\n",
    "\n",
    "# Creating ROC-AUC Curve\n",
    "fpr_t, tpr_t, thresholds_t = roc_curve(y_val, y_pred_prob_t)\n",
    "\n",
    "#DF for ROC-AUC information \n",
    "dfroc_t = pd.DataFrame({'False Positive Rate': fpr, \n",
    "                     'True Positive Rate': tpr,\n",
    "                    'Threshold': np.round(thresholds, 2)})\n",
    "\n",
    "\n",
    "#Plotting both lines on the same graph\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.plot(fpr_t, tpr_t, label='Boosting Model')\n",
    "plt.legend()\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC_AUC for both models\n",
    "print('LR Model ROC-AUC Score:', roc_auc_score(y_val, y_pred_prob))\n",
    "print('XG Model ROC-AUC Score:', roc_auc_score(y_val, y_pred_prob_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(best_gridcv, X_val, y_val, \n",
    "                                n_repeats=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'Feature': X_val.columns,\n",
    "                   'Importances': np.round(result['importances_mean'], 3),\n",
    "                   'importances_std': result['importances_std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df1.sort_values(by='Importances', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(df1, x='Feature', y='Importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDP Plots : Islolate + Interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDP looking directly at Tenure and its interaction with the target Churn\n",
    "\n",
    "from pdpbox.pdp import pdp_isolate, pdp_plot\n",
    "feature = 'tenure'\n",
    "\n",
    "isolated = pdp_isolate(\n",
    "    model=best_gridcv, \n",
    "    dataset=X_val, \n",
    "    model_features=X_val.columns, \n",
    "    feature=feature\n",
    ")\n",
    "\n",
    "pdp_plot(isolated, feature_name=feature, plot_lines=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDP on Tenure and Monthly Charges \n",
    "from pdpbox.pdp import pdp_interact, pdp_interact_plot\n",
    "\n",
    "features = ['tenure', 'MonthlyCharges']\n",
    "\n",
    "interaction = pdp_interact(\n",
    "    model=best_gridcv, \n",
    "    dataset=X_val, \n",
    "    model_features=X_val.columns, \n",
    "    features=features\n",
    ")\n",
    "\n",
    "pdp_interact_plot(interaction, plot_type='grid', feature_names=features);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gridcv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_test = best_gridcv.predict_proba(X_test)[:, -1]\n",
    "\n",
    "\n",
    "\n",
    "roc_auc_score(y_test, y_pred_prob_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(best_gridcv, 'model.joblib', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import sklearn\n",
    "import category_encoders as ce\n",
    "import xgboost\n",
    "print(f'joblib=={joblib.__version__}')\n",
    "print(f'scikit-learn=={sklearn.__version__}')\n",
    "print(f'category_encoders=={ce.__version__}')\n",
    "print(f'xgboost=={xgboost.__version__}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
